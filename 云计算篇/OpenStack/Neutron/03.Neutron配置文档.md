# Neutron配置文档


## ML2 Plug-in

### 架构

作为一个框架，允许OpenStack网络支持现实世界各种数据中心中使用的各种复杂的2层网络技术，区分出了2类可配置的驱动：

    1. Type drivers
    定义如何在技术上实现OpenStack网络，例如vxlan
    2. Mechanism drivers
    定义访问特定类型OpenStack网络的机制，例如OpenVSwitch mechanism driver

### 配置

#### 网络类型驱动

全部可用的网络类型驱动:

* Flat
* VLAN
* GRE
* VXLAN

供应商网络类型：

* Flat
* VLAN
* GRE
* VXLAN

项目网络类型：

* VLAN
* GRE
* VXLAN

#### 机制驱动

* Linux Bridge
* Open vSwitch
* SRIOV
* MacVTap
* L2 population
* Specialized， 譬如OpenDaylight

#### 扩展驱动

* QoS (当前未使用)
* port security (当前已使用)
* dns

#### 代理

##### L2 agent

L2 agent为OpenStack资源提供2层网络的连通性，运行在网络节点和计算节点上

* Open vSwitch agent (当前正在使用中的)
* Linux Bridge agent
* SRIOV Nic Switch agent
* MacVTap agent

##### L3 agent

L3 agent提供高级的3层服务，例如虚拟路由和浮动IP；需要和L2 agent同时运行

##### DHCP agent

DHCP agent负责管理DHCP和RADVD服务，它需要在同一个节点上运行L2 agent

##### Metadata agent

Metadata agent允许实例通过网络检索cloud-init元数据和用户数据，它需要在同一个节点上运行L2 agent

##### L3 metering agent

公有云厂商提供按流量计费的功能需要借助这个来实现？

#### 安全

L2 agent支持一些重要的安全配置：

* security group
* arp欺骗防护


## Address scopes

使用address scope后可以使外部网络和内部网络直接通信而不需要经过NAT


## Automatic allocation of network topologies

在创建虚拟机实例前，需要经过如下几步：

    1. Create a network
    2. Create a subnet
    3. Create a router
    4. Uplink the router on an external network
    5. Downlink the router on the previously created subnet

通过auto-allocated-topology扩展，我们可以省略以上几步来创建虚拟机实例，其实这个扩展就是帮我们自动创建一个网络(含子网)、一个路由器(连接到默认的外部网络上)


## Availability zones

network和router都有可用域
l3_ha = true 开启l3高可用


## BGP dynamic routing

pass


## High-availability for DHCP

配置/etc/neutron/neutron.conf， 开启dhcp ha
dhcp_agents_per_network = 3
在其他计算节点上安装neutron-dhcp-agent，然后进行适当配置即可
每增加一个neutron-dhcp-agent，都会在其管理的子网上增加一个dhcp port


## DNS integration

### 内部DNS解析

1. 配置/etc/neutron/plugins/ml2/ml2_conf.ini:
```
[ml2]
Extension_drivers = port_security,dns
```

2. 配置/etc/neutron/neutron.conf:
```
[DEFAULT]
dns_domain = example.org.
```

3. 重启neutron-server，就可以在port上面设置dns_name属性了，如果使用默认的dns_domain值可能出现设置dns_name不生效的情况，另外可能需要killall dnsmasq重启dhcp agent来清空之前的dns记录

**同一子网内的虚拟机实例可以通过这些内部域名进行通信，由dnsmasq负责解析**

### 整合外部DNS服务

pass


## Name resolution for instances

场景1：每个虚拟子网都使用单独的DNS解析器(会覆盖2的配置)

    我们在创建子网时可以指定多个DNS名称服务器

场景2：全部虚拟网络使用相同的DNS解析器

    配置/etc/neutron/dhcp_agent.ini，重启dhcp agent
```
[DEFAULT]
dnsmasq_dns_servers = 8.8.8.8, 8.8.4.4
```

场景3：全部虚拟网络使用DHCP agent所在主机配置的DNS解析器:

    配置/etc/neutron/dhcp_agent.ini，重启dhcp agent
```
[DEFAULT]
dnsmasq_local_resolv = True
```


## Distributed Virtual Routing with VRRP

基于VRRP(虚拟路由冗余协议)的分布式虚拟路由

使用DVR来实现HA的Open vSwitch支持使用VRRP来进行增强

master router定期发送heartbeat；当后备router在一段时间收不到heartbeat后，它就假设master router出问题了，然后在snat网络命名空间中的接口上配置IP地址，将自己提升为master router；在有多个后备router的情况下，根据VRRP规则选举出新的master router

### 配置示例

#### 控制节点配置

1.编辑/etc/neutron/neutron.conf，那么创建的router默认就是DVR和HA的：
```
[DEFAULT]
router_distributed = True
l3_ha = True
l3_ha_net_cidr = 169.254.192.0/18
max_l3_agents_per_router = 3
min_l3_agents_per_router = 2
```

#### 网络节点配置

1.编辑/etc/neutron/plugins/ml2/ml2_conf.ini：
```
[agent]
enable_distributed_routing = True
```

2.编辑/etc/neutron/l3_agent.ini：
```
[DEFAULT]
ha_vrrp_auth_password = password
agent_mode = dvr_snat
```

#### 计算节点配置

1.编辑/etc/neutron/plugins/ml2/ml2_conf.ini：
```
[agent]
enable_distributed_routing = True
```

2.编辑/etc/neutron/l3_agent.ini：
```
[DEFAULT]
agent_mode = dvr
```

### 已知限制

1.不能从仅为DVR、仅为HA或过时的router迁移为DVR且HA的，反向也不可以

2.在某些情况下，l2pop和DVR HR router不会以预期的方式进行交互，这个情况在仅为HA的router和l2pop下同样存在


## IPAM configuration

用于支持IP地址管理的驱动框架，支持与可选的IPAM实现或第三方管理系统的集成

### 基本配置

编辑/etc/neutron/neutron.conf来配置IPAM驱动:
```
[DEFAULT]
Ipam_driver = ipam-driver-name
```

### 已知限制

1.驱动程序接口被设计成支持为不同的subnet pool指定不同的驱动，然而目前的实现只支持在系统范围内指定一个IPAM驱动

2.第三方驱动程序必须提供自己的迁移机制来将现有的OpenStack安装转换为它的IPAM


## IPv6

### address_mode

Stateless Address Auto Configuration(SLAAC) 

    使用RA(路由通告)进行地址配置

DHCPv6-stateless 

    使用RA进行地址配置，使用DHCPv6获取可选信息

DHCPv6-stateful 

    使用DHCPv6进行地址配置和获取可选信息


## Load Balancer as a Service(LBaaS)

1. 通过neutron-lbaas插件提供负载均衡功能

2. V2在V1的基础上添加了listener概念

3. LBaaS v2有2个参考实现

* 基于HAProxy的agent，用于管理HAProxy的配置文件和守护进程

* Octavia，在hypervisor上创建负载均衡实例

### LBaas v2 Concepts

1. Load blancer

    负载均衡器占用一个neutron的网络端口，并具有从subnet分配的IP地址
    
2. Listener

    负载均衡器可以监听来自多个端口的请求，每个端口都由侦听器指定

3. Pool

    一个池包含被负载均衡的提供内容的成员列表

4. Member

    成员是负载均衡器后面的用于流量服务的服务器，每个成员都需要指定用于流量服务的IP地址和端口号

5. Health monitor

    成员可能会偶尔下线，健康监控器会从没有正确响应的成员那里分流流量


## Macvtap mechanism driver

### 架构图

![1](https://docs.openstack.org/newton/networking-guide/_images/config-macvtap-compute1.png)

![2](https://docs.openstack.org/newton/networking-guide/_images/config-macvtap-compute2.png)

### 已知限制

1. 只支持实例ports，dhcp和router port必须通过其他mechanism driver来实现

2. 只支持flat和vlan网络

3. 缺乏对安全组的支持包括基础的和反欺骗规则

4. 缺乏对L3高可用机制例如VRRP和DVR的支持

5. 只能连接计算资源

6. 实例迁移要求每个计算节点上的physical_interface_mapping配置选项具有相同的值


## MTU considerations

1. 通过配置支持巨型帧

2. 使用基于overlay协议的虚拟网络时，因为overlay封装占用一定字节，因此需要配置实例网卡的MTU，Linux系统可通过DHCP选项来自行配置，Windows系统可能需要经由cloud-init类似的agent来配置；举例子来说，以太网默认的MTU为1500，使用VXLAN时，VXLAN的封装需要耗费50字节，因此payload最多1450字节，需要配置MTU为1450


## Open vSwitch with DPDK datapath

1. 使用DPDK datapath来提供更低的延迟和更高的性能

2. Using vhost-user interface

### 已知限制

1. 只有在使用Libvirt计算驱动和KVM/QEMU hypervisor时才支持这个功能

2. Large pages required

3. 使用tap设备的服务性能会下降: 这些设备不支持DPDK，包括DVR、FWaaS和LBaaS


## Native Open vSwitch firewall driver

**历史上，Open vSwitch(OVS)不能直接与iptables交互以实现安全组，因此才插入一个网桥用于实现安全组**

在实例和物理物理基础设施间增加额外的组件会导致扩展性和性能问题，为了解决这个问题，OVS agent包含了一个可选的防火墙驱动，它通过OVS flows来实现安全组

### 先决条件

1. Kernel 版本大于等于4.3，并且支持conntrack

2. Kernel版本大于等于3.3小于4.3，需要编译OVS模块

### 启用原生OVS防火墙驱动

1. 在运行l2 ovs agent的节点上，编辑/etc/neutron/plugins/ml2/openvswitch_agent.ini:
```
[securitygroup]
firewall_driver = openvswitch
```

2. 重启ovs agent
```
service neutron-openvswitch-agent restart
```


## Quality of Service(QoS)

QoS被定义为能满足某些网络需求，如带宽、延迟、抖动和可靠性，以满足应用程序服务商和最终用户之间的服务水平协议(Service Level Agreement, SLA)

使用QoS来对port进行限制


## Role-Based Access Control(RBAC)

我们在创建网络时可以选择进行“共享”，其实也是使用RBAC policy来实现的：
```
neutron rbac-create --tenant-id <owner-tenant-id> --type network --target-tenant '*' --action access_as_shared <net-id> 
```

### 已知问题

当一个non-admin用户与其他租户共享了他的网络后，其他租户在此网络上创建的ports，他是看不到的也不能删除，这是由于neutron数据库查询的限制导致的，除非该用户时管理员或服务用户


## Service function chaining

服务链(SFC)本质上是基于策略的路由(policy-based routing, PBR)的软件定义网络(SDN)版本

服务链本质上是模拟通过电缆将一系列物理网络设备连接在一起

### Architecture

Port chain或者 service function chain，由以下2部分组成：

1. 定义service function序列的一组ports

2. 一组流分类器，用于指定进入链的分类流量

一个service function包含2个port，一个作为ingress port一个作为egress port，这2个port可以是同一个port

Port chain是单向的服务链

![3](https://docs.openstack.org/mitaka/networking-guide/_images/port-chain-architecture-diagram.png)

![4](https://docs.openstack.org/mitaka/networking-guide/_images/port-chain-diagram.png)

### Resources

1. Port chain

    一条port chain由多个port pair groups的序列组成，每个port pair group表示chain上的一跳

    一个port pair的group表示多个提供相同功能的server functions

    一条port chain可以包含多个流分类器

2. Port pair group

    一个port pair group包含一个或多个port pair，多个port pairs支持在一组等效的service functions上进行负载均衡

3. Port pair

    一个port pair代表一个service function实例，包含一个ingress和一个egress port，如果ingress port和egress port相同，那么就是双向端口

4. Flow classifier

    通过源端口、源地址、目的端口、目的地址等一系列属性来标识一个特定的flow


## Using SR-IOV functionality

### 已知限制

1. Qos max_burst_kbps 不支持

2. 安全组不支持，sriov-agent只能配置firewall_driver = neutron.agent.firewall.NoopFirewallDriver

3. 没有OpenStack Dashboard整合，只能通过CLI或者API来创建sriov ports

4. 使用sriov ports的实例不支持热迁移


## Subnet pools

pass
